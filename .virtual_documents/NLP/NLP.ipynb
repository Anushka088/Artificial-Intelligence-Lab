import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download("taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle").tagdict['weather'] = 'NN'
text="""Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn\'t eat cardboard"""



sent=nltk.sent_tokenize(text)
print(sent)


import re
#text="SaaaaMMM"
print(text.lower())
text = re.sub(r"[^a-z\'A-Z]", " ", text.lower())
text



words= text.split()

print(words)


from nltk.corpus import stopwords
print(stopwords.words("english"))


words = [w for w in words if w not in stopwords.words("english")]
print(words)

#for k in words:
 # for l in k:
  #  if l not in stopwords.words("english"):

   #   print(l)


from nltk.stem.porter import PorterStemmer
# Reduce words to their stems
stemmed = [PorterStemmer().stem(w) for w in words]
print(stemmed)



from nltk.stem.wordnet import WordNetLemmatizer
# Reduce words to their root form
lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]
print(lemmed)


from nltk import pos_tag,RegexpParser

tagged=pos_tag(lemmed)
print(tagged)
chunker = RegexpParser("""
NP: {<DT>?<JJ>*<NN.*>}    # Noun Phrases
  P: {<IN>}                 # Prepositions
  V: {<VB.*>}               # Verbs
  PP: {<P><NP>}             # Prepositional Phrases
  VP: {<V><NP|PP>*}         # Verb Phrases""")
output = chunker.parse(tagged)
print("After Extracting", output)




!pip install scipy



import nltk
nltk.download('averaged_perceptron_tagger')



import nltk
nltk.download('maxent_ne_chunker')


import nltk
nltk.download('words')


!pip install spacy
!python -m spacy download en_core_web_sm



!pip install --upgrade numpy scipy




